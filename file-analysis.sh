#!/bin/bash

# Output Files Analysis Script
# Analyzes generated files from Common Crawl processing jobs

echo "=========================================="
echo "OUTPUT FILES ANALYSIS REPORT"
echo "=========================================="
echo "Generated: $(date)"
echo ""

# Function to format bytes in human readable format
format_bytes() {
    local bytes=$1
    if [ $bytes -ge 1073741824 ]; then
        echo "$(echo "scale=2; $bytes / 1073741824" | bc -l) GB"
    elif [ $bytes -ge 1048576 ]; then
        echo "$(echo "scale=2; $bytes / 1048576" | bc -l) MB"
    elif [ $bytes -ge 1024 ]; then
        echo "$(echo "scale=2; $bytes / 1024" | bc -l) KB"
    else
        echo "$bytes bytes"
    fi
}

echo "ğŸ“ OUTPUT DIRECTORIES ANALYSIS"
echo "----------------------------------------"

# Check common output directories
OUTPUT_DIRS=("output" "202104-output" "generated_scripts" "miniconda3" ".conda_env")

TOTAL_FILES=0
TOTAL_SIZE=0

for dir in "${OUTPUT_DIRS[@]}"; do
    if [ -d "$dir" ]; then
        echo ""
        echo "ğŸ“‚ Directory: $dir"
        echo "   â”œâ”€â”€ Exists: âœ…"
        
        # Count files
        FILE_COUNT=$(find "$dir" -type f 2>/dev/null | wc -l)
        echo "   â”œâ”€â”€ Files: $FILE_COUNT"
        
        # Calculate total size
        DIR_SIZE=$(find "$dir" -type f -exec stat -c%s {} + 2>/dev/null | awk '{sum+=$1} END {print sum+0}')
        READABLE_SIZE=$(format_bytes $DIR_SIZE)
        echo "   â”œâ”€â”€ Size: $READABLE_SIZE"
        
        # Show file types
        echo "   â””â”€â”€ File types:"
        find "$dir" -type f 2>/dev/null | grep -o '\.[^.]*$' | sort | uniq -c | sort -nr | head -5 | while read count ext; do
            echo "       â€¢ $ext: $count files"
        done
        
        TOTAL_FILES=$((TOTAL_FILES + FILE_COUNT))
        TOTAL_SIZE=$((TOTAL_SIZE + DIR_SIZE))
    else
        echo ""
        echo "ğŸ“‚ Directory: $dir"
        echo "   â””â”€â”€ Status: âŒ Not found"
    fi
done

echo ""
echo "ğŸ“Š SUMMARY STATISTICS"
echo "----------------------------------------"
echo "ğŸ—‚ï¸  Total output files: $TOTAL_FILES"
echo "ğŸ’¾ Total output size: $(format_bytes $TOTAL_SIZE)"

# Check for SLURM output files
echo ""
echo "ğŸ“‹ SLURM LOG FILES"
echo "----------------------------------------"

SLURM_OUT=$(find . -maxdepth 1 -name "*.out" 2>/dev/null | wc -l)
SLURM_ERR=$(find . -maxdepth 1 -name "*.err" 2>/dev/null | wc -l)
SLURM_SIZE=$(find . -maxdepth 1 -name "*.out" -o -name "*.err" 2>/dev/null | xargs stat -c%s 2>/dev/null | awk '{sum+=$1} END {print sum+0}')

echo "ğŸ“„ SLURM output files (.out): $SLURM_OUT"
echo "ğŸ“„ SLURM error files (.err): $SLURM_ERR"
echo "ğŸ’¾ SLURM logs total size: $(format_bytes $SLURM_SIZE)"

# Recent files analysis
echo ""
echo "â° RECENT FILE ACTIVITY"
echo "----------------------------------------"

echo "ğŸ“… Files created in last hour:"
RECENT_1H=$(find . -type f -newermt "1 hour ago" 2>/dev/null | wc -l)
echo "   â””â”€â”€ Count: $RECENT_1H files"

echo "ğŸ“… Files created in last 6 hours:"
RECENT_6H=$(find . -type f -newermt "6 hours ago" 2>/dev/null | wc -l)
echo "   â””â”€â”€ Count: $RECENT_6H files"

echo "ğŸ“… Files created today:"
RECENT_TODAY=$(find . -type f -newermt "today" 2>/dev/null | wc -l)
echo "   â””â”€â”€ Count: $RECENT_TODAY files"

# Check for specific output patterns
echo ""
echo "ğŸ” OUTPUT FILE PATTERNS"
echo "----------------------------------------"

# Look for parquet files
PARQUET_COUNT=$(find . -name "*.parquet" 2>/dev/null | wc -l)
PARQUET_SIZE=$(find . -name "*.parquet" -exec stat -c%s {} + 2>/dev/null | awk '{sum+=$1} END {print sum+0}')
echo "ğŸ“Š Parquet files: $PARQUET_COUNT ($(format_bytes $PARQUET_SIZE))"

# Look for CSV files
CSV_COUNT=$(find . -name "*.csv" 2>/dev/null | wc -l)
CSV_SIZE=$(find . -name "*.csv" -exec stat -c%s {} + 2>/dev/null | awk '{sum+=$1} END {print sum+0}')
echo "ğŸ“ˆ CSV files: $CSV_COUNT ($(format_bytes $CSV_SIZE))"

# Look for JSON files
JSON_COUNT=$(find . -name "*.json" 2>/dev/null | wc -l)
JSON_SIZE=$(find . -name "*.json" -exec stat -c%s {} + 2>/dev/null | awk '{sum+=$1} END {print sum+0}')
echo "ğŸ“‹ JSON files: $JSON_COUNT ($(format_bytes $JSON_SIZE))"

# Show largest files
echo ""
echo "ğŸ“ LARGEST OUTPUT FILES"
echo "----------------------------------------"
find . -path "./output*" -name "*.parquet" 2>/dev/null | \
    xargs ls -la 2>/dev/null | \
    sort -k5 -nr | \
    head -10 | \
    while read perm links owner group size month day time filename; do
        readable=$(format_bytes $size)
        echo "ğŸ’ $(basename $filename) ($readable)"
    done

# If no parquet files found, show CSV files
if [ $(find . -path "./output*" -name "*.parquet" 2>/dev/null | wc -l) -eq 0 ]; then
    echo "No parquet files found in output directories"
    find . -path "./output*" -name "*.csv" 2>/dev/null | \
        xargs ls -la 2>/dev/null | \
        sort -k5 -nr | \
        head -5 | \
        while read perm links owner group size month day time filename; do
            readable=$(format_bytes $size)
            echo "ğŸ’ $(basename $filename) ($readable)"
        done
fi

# Disk space analysis
echo ""
echo "ğŸ’¿ DISK SPACE ANALYSIS"
echo "----------------------------------------"

# Current directory usage
CURRENT_SIZE=$(du -sb . 2>/dev/null | awk '{print $1}')
echo "ğŸ“ Current directory total: $(format_bytes $CURRENT_SIZE)"

# Available space
AVAIL_SPACE=$(df . | tail -1 | awk '{print $4 * 1024}')
echo "ğŸ’½ Available space: $(format_bytes $AVAIL_SPACE)"

# Calculate percentage used
USED_PERCENT=$(df . | tail -1 | awk '{print $5}' | tr -d '%')
echo "ğŸ“Š Disk usage: $USED_PERCENT%"

echo ""
echo "=========================================="
echo "ğŸ’¡ To refresh this report, run: ./file-analysis.sh"
echo "ğŸ—‘ï¸  To clean up logs, run: ./cleanup.sh"
echo "ğŸ“‚ To check specific directory: ls -lah DIRNAME"
echo "=========================================="